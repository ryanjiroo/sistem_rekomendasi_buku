# -*- coding: utf-8 -*-
"""rekomendasi_buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KZs_AqxDC1xyGo3JMaPV7XftR6DWNzMW

# Rekomendasi Buku Berbasis Content-Based dan Collaborative Filtering

# Dataset dan Referensi

**Dataset:** [Books Dataset](https://www.kaggle.com/datasets/saurabhbagchi/books-dataset)

* Berisi lebih dari 700.000 interaksi pengguna dan 180.000+ resep
* Dua file utama digunakan:

  * `books.csv` â€” data buku yang ada
  * `ratings.csv` â€” rating yang diberikan user kepada setiap buku

# Import Library
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
import joblib
import pickle
import os

from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import mean_squared_error

import warnings
warnings.filterwarnings('ignore')

"""# Load Data"""

df_books = pd.read_csv('books.csv', encoding='latin-1', delimiter=';', on_bad_lines='skip')
df_ratings = pd.read_csv('ratings.csv', encoding='latin-1', delimiter=';', on_bad_lines='skip')

"""# EDA

## Ekplorasi Umum
"""

df_books.shape

df_ratings.shape

df_books.info()

df_ratings.info()

df_books.head()

df_ratings.head()

df_books.describe()

df_ratings.describe()

df_books.isnull().sum()

df_ratings.isnull().sum()

df_books.duplicated().sum()

df_ratings.duplicated().sum()

"""## Visualisasi"""

plt.figure(figsize=(10, 6))
sns.countplot(x='Book-Rating', data=df_ratings)
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

# Gabungkan data berdasarkan ISBN
df = df_ratings.merge(df_books, on='ISBN', how='left')

# Hanya ambil rating > 0 (rating aktual)
df_filtered = df[df['Book-Rating'] > 0]

# Hitung rata-rata dan jumlah rating per buku
avg_rating = df_filtered.groupby('Book-Title')['Book-Rating'].agg(['mean', 'count'])

# Filter buku dengan minimal 50 rating
popular_books = avg_rating[avg_rating['count'] >= 50]

# Urutkan dari rata-rata rating tertinggi
top_books = popular_books.sort_values(by='mean', ascending=False).head(10)

# Tampilkan hasil sebagai DataFrame
print("Top 10 Buku dengan Rata-Rata Rating Tertinggi (min 50 rating):")
print(top_books)

# Visualisasi
top_books['mean'].sort_values().plot(kind='barh', figsize=(10, 6), color='skyblue')
plt.title('Top 10 Buku dengan Rata-Rata Rating Tertinggi (â‰¥ 50 rating)')
plt.xlabel('Rata-Rata Rating')
plt.ylabel('Judul Buku')
plt.tight_layout()
plt.show()

"""# Preprocessing"""

df_books['Year-Of-Publication'] = pd.to_datetime(df_books['Year-Of-Publication'], errors='coerce', format='%Y')

df_books.info()

df_books.dropna(inplace=True)

df_books.isnull().sum()

"""# Feature Engineering - Content-Based Representation"""

df_books['text'] = (
    df_books['Book-Title'].astype(str) + ' ' +
    df_books['Book-Author'].astype(str) + ' ' +
    df_books['Publisher'].astype(str)
).str.lower()

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(df_books['text'])

print(f"Shape of TF-IDF Matrix: {tfidf_matrix.shape}")

from sklearn.neighbors import NearestNeighbors

# TF-IDF dari sebelumnya
# tfidf_matrix = tfidf.fit_transform(df_books['text'])

# Buat model Nearest Neighbors dengan cosine distance (1 - cosine similarity)
model_nn = NearestNeighbors(metric='cosine', algorithm='brute')
model_nn.fit(tfidf_matrix)

df_books = df_books.reset_index(drop=True)
indices = pd.Series(df_books.index, index=df_books['Book-Title'].str.lower()).drop_duplicates()

def recommend_fast(title, model=model_nn, tfidf_matrix=tfidf_matrix, df=df_books, top_n=10):

    title = title.lower()
    if title not in indices:
        return f"Buku '{title}' tidak ditemukan."

    idx = indices[title]
    tfidf_vector = tfidf_matrix[idx]

    distances, indices_nn = model.kneighbors(tfidf_vector, n_neighbors=top_n + 1)
    recommended_idx = indices_nn[0][1:]  # skip self

    return df[['Book-Title', 'Book-Author']].iloc[recommended_idx]

recommend_fast("The Hobbit")

"""# Collaborative Filtering"""

import numpy as np
print(np.__version__)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dot, Flatten

# Hyperparameters
num_users = df_ratings['User-ID'].nunique()
num_books = df_books['ISBN'].nunique()
embedding_dim = 50

# Input layers
user_input = Input(shape=(1,))
book_input = Input(shape=(1,))

# Embedding layers
user_embedding = Embedding(input_dim=num_users+1, output_dim=embedding_dim)(user_input)
book_embedding = Embedding(input_dim=num_books+1, output_dim=embedding_dim)(book_input)

# Dot product of embeddings
dot_product = Dot(axes=2)([user_embedding, book_embedding])
dot_product = Flatten()(dot_product)

model = Model(inputs=[user_input, book_input], outputs=dot_product)
model.compile(optimizer='adam', loss='mse')

from sklearn.preprocessing import LabelEncoder

user_enc = LabelEncoder()
book_enc = LabelEncoder()

df_ratings = df_ratings[df_ratings['Book-Rating'] > 0]

df_ratings['user'] = user_enc.fit_transform(df_ratings['User-ID'])
df_ratings['book'] = book_enc.fit_transform(df_ratings['ISBN'])

X = df_ratings[['user', 'book']].values
y = df_ratings['Book-Rating'].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit model ke train set
model.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=5, batch_size=64, verbose=1)

"""## Evaluasi model"""

y_pred = model.predict([X_test[:, 0], X_test[:, 1]]).flatten()

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE: {rmse:.4f}")

"""## Inference (Rekomendasi Top-N Buku untuk Seorang User)"""

import numpy as np
import pandas as pd

def recommend_books_for_user(user_id_original, df_ratings, df_books, user_encoder, book_encoder, model, top_n=10, max_candidates=2000):
    # Cek apakah user dikenal
    if user_id_original not in user_encoder.classes_:
        return f"âŒ User ID {user_id_original} tidak ditemukan dalam data training."

    # Encode user
    user_id_encoded = user_encoder.transform([user_id_original])[0]

    # Ambil semua ISBN buku unik
    all_books_isbn = df_books['ISBN'].unique()

    # Filter ISBN yang dikenal encoder
    known_books_isbn = [isbn for isbn in all_books_isbn if isbn in book_encoder.classes_]

    # Encode ISBN buku
    known_books_encoded = book_encoder.transform(known_books_isbn)

    # Ambil ISBN buku yang sudah dirating user (dikenal encoder saja)
    rated_books = df_ratings[df_ratings['User-ID'] == user_id_original]['ISBN'].values
    rated_books_encoded = book_encoder.transform([isbn for isbn in rated_books if isbn in book_encoder.classes_])

    # Buku kandidat = semua buku yang belum pernah dirating
    candidate_books_encoded = [b for b in known_books_encoded if b not in rated_books_encoded]

    # ðŸ”§ Batasi jumlah kandidat jika terlalu banyak
    if len(candidate_books_encoded) > max_candidates:
        candidate_books_encoded = np.random.choice(candidate_books_encoded, size=max_candidates, replace=False)

    if len(candidate_books_encoded) == 0:
        return f"âœ… User {user_id_original} sudah merating semua buku yang tersedia."

    # Siapkan array user sebanyak kandidat buku
    user_array = np.full(len(candidate_books_encoded), user_id_encoded)

    # Prediksi dengan batch_size agar cepat
    predictions = model.predict(
        [user_array, np.array(candidate_books_encoded)],
        batch_size=1024,
        verbose=0
    ).flatten()

    # Ambil top-N
    top_indices = predictions.argsort()[-top_n:][::-1]
    top_books_encoded = np.array(candidate_books_encoded)[top_indices]
    top_ratings = predictions[top_indices]

    # Decode ISBN dan ambil judul
    top_books_isbn = book_encoder.inverse_transform(top_books_encoded)
    top_books_df = df_books[df_books['ISBN'].isin(top_books_isbn)].drop_duplicates('ISBN')

    # Gabungkan hasil akhir
    result = top_books_df.set_index('ISBN').loc[top_books_isbn]
    result['Predicted Rating'] = top_ratings

    return result.reset_index()

recommend_books_for_user(
    user_id_original=276726,
    df_ratings=df_ratings,
    df_books=df_books,
    user_encoder=user_enc,
    book_encoder=book_enc,
    model=model,
    top_n=5  # top 5 rekomendasi
)

model.save('recommender_model.h5')

import joblib

joblib.dump(user_enc, 'user_encoder.pkl')
joblib.dump(book_enc, 'book_encoder.pkl')